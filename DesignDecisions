It is mentioned billions of requests per day,
and I do not know how many billions.. 2 billion or 20 billion ?
Secondly, how many unique customers are there?


Input data:
{
"customerID":1,
"tagID":2,
"userID":"aaaaaaaa-bbbb-cccc-1111-222222222222",
"remoteIP":"123.234.56.78",
"timestamp":1500000000
}

Each row will be roughly (32 chars, 16 chars, 64 chars, 45 chars allowing IPv6, 16 chars)
- roughly a 180 chars per row, or per write

Assuming ASCII, 1 billion * 180 * 2 => 360 billion bytes => 360 GB per day
If instead of a billion, it is 100 billion, we are talking about a 32 TB per day. So to know the actual scale is important

Assumptions:
Input is a few billions of requests per day
We do not know about the spike of data, a scalable service deployment should be able to handle spikes.

Trade offs:
If the database can hold a few Terabytes of data, and we are concerned only about a history of a weeks statistics,
then historical data can be moved out from the SQL database periodically. This also means downtime needs to be handled
with replicas or secondary backups.

If we dont want downtime at all, and want more than a weeks older data statistics, or can expect adhoc date statistics,
then we need to maintain data in the DB. So we need a database that supports horizontal scaling for storing the terabytes of data
and also supporting any date statistics, for example statistics on last Christmas day (or any appropriate usecase). In such cases,
the queries can reach exact shard if we horizontally shard the database and we can get faster results.

For demo purpose, I will focus more on a quick solution to see coding abilities. Hence using an embedded H2 DB, only for testing now.

Notes:
Filtering & Pagination API

Failure scenarios:
Thundering Herd problem - Use Exponential back off with Jitter

Further improvements:
Include health check endpoints /health
With scale, isn't it better to introduce a queue and persist the writes to the DB. Also, Pick a DLQ to handle main Queue failures as well.
But of course it is an overkill if the scale is not large since also failure of the queue needs to be handled.

